{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tF5oVtBA8Ttk"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.content/competitiondata # this is used to create the content for the\n",
        "# custom test data to generate the submission csv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "2gToprcS4bUe",
        "outputId": "0e4d1786-8e63-4c43-8a2b-829640c5059d"
      },
      "outputs": [],
      "source": [
        "from google.colab import files # used in colab for uploading files\n",
        "files.upload()  # Used to upload the model.pth and the kaggle.json API for volab\n",
        "\n",
        "# Make sure kaggle.json is in the location ~/.kaggle/kaggle.json\n",
        "# all these are commands to get the custom data set for the CIFAR-10\n",
        "# it isnt neccesary if the test data is something else.\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c deep-learning-mini-project-spring-24-nyu\n",
        "!unzip deep-learning-mini-project-spring-24-nyu.zip -d competitiondata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gwhSAVlDCXhc"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "from torch.cuda.amp import GradScaler, autocast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUF5ls1H9Drf",
        "outputId": "1eae7c8b-6c2d-46d1-e34e-f97e58e30e31"
      },
      "outputs": [],
      "source": [
        "# this is the residual block adapted from https://github.com/drgripal/resnet-cifar10\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=None, padding=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=padding, bias=False) # trying same padding\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.gelu = nn.GELU() # adding in a gelu layer for testing\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=padding, bias=False) # valid padding trying\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.downsample = downsample # this is a downsample that was used in teh residual block default is none\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        identity = nn.AdaptiveAvgPool2d(out.size()[2])(identity)\n",
        "        out += identity\n",
        "        out = self.gelu(out) # trying out a gelu for one of the blocks\n",
        "\n",
        "        return out\n",
        "\n",
        "class ModifiedResNet(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes=10):\n",
        "        super(ModifiedResNet, self).__init__()\n",
        "        self.in_channels = 16 # number of output channels\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.dropout = nn.Dropout(0.432); # dropout layer probability\n",
        "        self.dropoutlow = nn.Dropout(0.287) # lower dropout prob\n",
        "        self.bn1 = nn.BatchNorm2d(16) # a smaller batch norm\n",
        "        self.maxpool1 = nn.AdaptiveMaxPool2d(24) # smaller max pool\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self._make_layer(block, 32, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 64, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 115, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 256, layers[3], stride=2)\n",
        "        # Adjust the layer configuration to stay under 5 million parameters\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((3, 3))\n",
        "        self.fc = nn.Linear(256 * 3 * 3, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
        "        # this function creates the residual blcoks and then\n",
        "        # adds in the downsampling if neccesary\n",
        "        downsample = None\n",
        "        if stride != 1 or self.in_channels != out_channels:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "            )\n",
        "        layers = []\n",
        "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
        "        self.in_channels = out_channels\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(out_channels, out_channels))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # the forward pass\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.dropoutlow(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.maxpool1(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.dropoutlow(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "model = ModifiedResNet(ResidualBlock, [3, 4, 11, 2]).to('cuda')\n",
        "# the model being created\n",
        "\n",
        "# Use torchsummary for a detailed summary and parameter count\n",
        "from torchsummary import summary\n",
        "summary(model, (3, 32, 32))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3x2ARAQ7LiV"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),  # Add vertical flip\n",
        "    transforms.RandomRotation(10),    # Add random rotation\n",
        "    transforms.RandomCrop(32, 4),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Add random translation\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Add color jitter\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "# Use the function to load the data and the test data\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Dymr51KO-NKC"
      },
      "outputs": [],
      "source": [
        "#defining model, optimizer, regularization, hyperparameters\n",
        "\n",
        "# hyperparameters\n",
        "epochs = 200\n",
        "lr = 1e-4\n",
        "grad_accumulation = 3\n",
        "model_save = 5\n",
        "# use modified ResNet model\n",
        "GPU = True\n",
        "\n",
        "# define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss() # the loss function used\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0.001)\n",
        "#optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.1) tried SGD with diff momentums\n",
        "scaler = GradScaler(enabled=GPU) # the scaler for mixed avergae precision\n",
        "#scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2) # the LR scheduler\n",
        "\n",
        "# loading in the functions from the best_model deep for training and testing\n",
        "# otherwise just the model can be used with training enabled\n",
        "# uncomment these lines to load from the best_model.pth\n",
        "#checkpoint = torch.load('best_model_deep.pth')\n",
        "#model.load_state_dict(checkpoint['model_state_dict'])\n",
        "#optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "#scaler.load_state_dict(checkpoint['scaler'])\n",
        "#scheduler.load_state_dict(checkpoint['scheduler'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "kr3vZh8k-Z0B",
        "outputId": "f0aafd20-3f9c-4399-b18f-83d7b3a925a3"
      },
      "outputs": [],
      "source": [
        "# training model\n",
        "import time\n",
        "\n",
        "# train model and keep a tab on running loss\n",
        "for epoch in range(epochs):\n",
        "    start_time = time.time()\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data[0].cuda(), data[1].cuda()\n",
        "\n",
        "        # amap for speeding up training\n",
        "        with autocast(enabled=GPU):\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        # grad accumulation for speeding training\n",
        "        if (i + 1) % grad_accumulation == 0:\n",
        "          scaler.step(optimizer)\n",
        "          scaler.update()\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 0:    # print loss after 200 iters and first iter\n",
        "            print(f'[{epoch + 1}, {i + 1}] loss: {running_loss / 200:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    # save model incase something gooes wrong\n",
        "    if epoch % model_save == 0:\n",
        "        print(\"just saved model incase something goes wrong\")\n",
        "        model.cpu()  # Move model to CPU\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            'scheduler': scheduler.state_dict(),\n",
        "            'scaler': scaler.state_dict(),\n",
        "            }, \"model.pth\")\n",
        "        model.cuda()  # Move model back to GPU if further training is needed\n",
        "\n",
        "\n",
        "    end_time = (time.time() - start_time ) // 1\n",
        "    print(f\"finished epoch {epoch + 1} in {end_time} seconds\" )\n",
        "\n",
        "print('Finished Training')\n",
        "print(\"just saved final model incase something goes wrong\")\n",
        "model.cpu()  # Move model to CPU\n",
        "# save final model\n",
        "torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            'scheduler': scheduler.state_dict(),\n",
        "            'scaler': scaler.state_dict(),\n",
        "            }, \"model.pth\")\n",
        "model.cuda()  # Move model back to GPU if further training is needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "G0cfvGyH_Eor",
        "outputId": "998f3e6e-47ea-41d7-bafc-2de27997715e"
      },
      "outputs": [],
      "source": [
        "# testing data for colab if the testing is done on the custom data\n",
        "# testing data code adopted from https://github.com/hzhao20/DLMiniproject/blob/main/GenerateCSV.py.\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# unpickle function\n",
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict\n",
        "\n",
        "# load\n",
        "transform = transforms.Compose([\n",
        "    #transforms.ToPILImage(),# this is used to transform to an image and should be uncommented if using custom data\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # use for the same noramlization as test\n",
        "])\n",
        "\n",
        "\n",
        "testdata = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "test_loader = DataLoader(testdata, batch_size=64, shuffle=False)\n",
        "# test data can be commented out if not custom data\n",
        "\n",
        "\"\"\"\n",
        "All this here is loading for the test data that was from teh competition\n",
        "test_data_dict = unpickle('/content/competitiondata/cifar_test_nolabels.pkl')\n",
        "test_images = test_data_dict[b'data']\n",
        "test_ids = test_data_dict[b'ids']\n",
        "\n",
        "\n",
        "# transform\n",
        "test_images = test_images.reshape(len(test_images), 3, 32, 32).transpose(0, 2, 3, 1)  # reshaping data to the proper formats\n",
        "\n",
        "# preprocess\n",
        "\n",
        "\n",
        "test_images = torch.stack([transform(img) for img in test_images])\n",
        "\n",
        "# data loader\n",
        "#test_loader = DataLoader(TensorDataset(test_images, torch.tensor(test_ids)), batch_size=64, shuffle=False)\n",
        "# the top line can be uncommented if the testing is done on the custom data\n",
        "# else keeping it commented should load the original CIFAR-10 data set\n",
        "\"\"\"\n",
        "\n",
        "# load model\n",
        "model = model.to(device) # prolly on cuda\n",
        "model.eval()\n",
        "\n",
        "# predict\n",
        "correct = 0\n",
        "total = 0\n",
        "predicted_labels = []\n",
        "# labels should be replaced with _ if CIFAR 10 data\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        predicted_labels.extend(preds.cpu().numpy())\n",
        "        total += labels.size(0) # this line shoiuld be commented out if competition data\n",
        "        correct += (preds == labels).sum().item() # this line should be commented out if competition data\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')\n",
        "# this should be commented it custom data\n",
        "\n",
        "# generate CSV\n",
        "\"\"\"\n",
        "uncomment these lines if running to generate the competiion csv\n",
        "submission_df = pd.DataFrame({\n",
        "    'ID': test_ids,\n",
        "    'Labels': predicted_labels\n",
        "})\n",
        "\n",
        "predicted_labels = np.array(predicted_labels)\n",
        "\n",
        "submission_df.to_csv('submission.csv', index=False)\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
